\chapter{Related Work}
\label{chap:related_work}


\section{Semantic Table Interpretation and Semantic Annotation}

Semantic Table Interpretation (STI) aims to map tabular content to structured semantics by linking cells, columns, and relationships to a target knowledge graph or ontology. Early approaches formulated annotation as collective inference over cell entities, column types, and inter-column relations, leveraging global coherence to resolve lexical ambiguity under sparse context. Limaye et al. (2010) introduced probabilistic models that jointly assign entities, types, and relations, demonstrating improved robustness over isolated string matching~\cite{limaye2010annotating}. Subsequent systems incorporated web-scale evidence to infer column semantics despite missing or noisy metadata~\cite{venetis2011recovering}.

Parallel efforts aligned web tables with knowledge bases to publish linked data. Mulwad et al. (2013) employed message-passing joint inference to generate RDF triples by exploiting signals from the Linked Open Data cloud~\cite{mulwad2013semantic}. Ritze et al. (2015) systematized evaluation for table-to-knowledge-base matching by constructing gold standards that characterize both schema-level and entity-level correspondences between large web table corpora and DBpedia~\cite{ritze2015matching}.

The SemTab challenge series has since established shared tasks, datasets, and evaluation protocols for matching tabular data to knowledge graphs, decomposing STI into cell entity annotation (CEA), column type annotation (CTA), and column property annotation (CPA)~\cite{jimenez2020semtab}. These benchmarks foreground practical challenges including missing captions, ambiguous headers, and noisy values, thereby emphasizing annotation under weak supervision and heterogeneous evidence.

\section{Column Type Annotation: From Feature Engineering to Representation Learning}

Column type annotation (CTA) assigns a semantic class to an entire column and is closely related to semantic labeling, which maps attributes from heterogeneous sources to ontology classes. Pham et al. (2016) proposed a domain-independent semantic labeling method combining multiple similarity signals including distributional evidence from column values to reduce reliance on hand-crafted, domain-specific rules~\cite{pham2016semantic}. Although feature-based systems improved generality, they remain dependent on engineered similarity functions and representative training data.

Representation learning has substantially advanced column semantic type detection by inducing richer encodings from column values and metadata. Sherlock (Hulsebos et al., 2019) introduced a multi-input neural architecture trained on hundreds of thousands of columns, combining learned representations with multiple feature views derived from values and headers~\cite{hulsebos2019sherlock}. ColNet (Chen et al., 2019) embedded web table column semantics for type prediction while integrating knowledge-base signals, reporting improvements on datasets with DBpedia-backed type spaces~\cite{chen2019colnet}.

Pre-trained language models (PLMs) subsequently shifted table understanding toward transfer learning. TURL (Deng et al., 2022) introduced structure-aware Transformer pretraining for relational web tables, learning universal representations that transfer across multiple table understanding tasks~\cite{deng2022turl}. Doduo (Suhara et al., 2022) framed column type and relation prediction as multi-task learning atop PLMs, achieving strong benchmark results with compact token budgets per column~\cite{suhara2022annotating}.

Despite these advances, most CTA systems and benchmarks assume label spaces derived from general-purpose knowledge graphs (e.g., DBpedia or Wikidata) and target web tables where entity linking signals are abundant. Domain-specific settings instead require mapping columns to specialized ontologies characterized by deep hierarchies, fine granularity, and long-tail concepts, often under scarce supervision. Ontology-grounded CTA therefore benefits from decision procedures that explicitly enforce structural consistency rather than unconstrained open-vocabulary classification.

\section{Ontology-Aware Search and Structural Constraints}

A central challenge in ontology-grounded annotation lies in ensuring that predicted labels are both semantically plausible and structurally valid within a given ontology. In semantic parsing, typing and grammar constraints have long served to guarantee validity and control combinatorial explosion during search. Pasupat and Liang (2015) applied strong typing constraints and constrained search to generate executable logical forms over semi-structured tables, demonstrating that structural constraints improve reliability~\cite{pasupat2015compositional}.

Related principles appear in ontology matching, where systems align classes across different ontologies. Recent work integrates LLMs into pipelines combining retrieval-based candidate generation with bounded decision steps. OLaLa (Hertling et al., 2023) investigated zero-shot and few-shot prompting with multiple LLMs for ontology matching, emphasizing prompt design, candidate selection, and workflow integration~\cite{hertling2023olala}. LLMs4OM (Babaei et al., 2024) evaluated LLM effectiveness by coupling retrieval and matching modules~\cite{babaei2024llms4om}. MILA (Taboada et al., 2025) combined retrieval with selective LLM prompting within a search strategy that concentrates LLM calls on borderline cases, achieving strong performance on OAEI tasks while controlling computational cost~\cite{taboada2025ontology}.

Although ontology matching differs from CTA, the underlying principle transfers directly: generate a constrained candidate set from the ontology(optionally augmented by retrieval), then employ an LLM as a local ranker or verifier within a structured search procedure. Such formulations improve auditability and preserve ontological validity, both essential for ontology-grounded CTA.

\section{Large Language Models for Table Annotation and Knowledge Engineering}

Large language models (LLMs) have reinvigorated semantic annotation by providing strong zero-shot semantic inference and robustness to lexical variation. Nevertheless, empirical studies indicate that performance on table-related tasks depends critically on task formulation, context construction, and constraint handling. Analyses of SemTab systems reveal substantial variation in how retrieval, linking, and reasoning components are composed, even under standardized evaluation protocols~\cite{hassanzadeh2024results}.

Recent work applies LLMs directly to schema-level annotation tasks. Korini and Bizer (2024) studied column property annotation (CPA) with LLMs, focusing on predicting semantic relationships between columns under candidate constraints~\cite{korini2024column}. For CTA, workshop studies examine how prompting and context selection affect column type predictions~\cite{babamahmoudi2150improving}. Collectively, these results indicate that LLMs provide strong semantic priors, yet naive prompting can yield invalid or inconsistent outputs when the label space comprises a deep, domain-specific ontology rather than a small set of generic types. Concurrently, community resources increasingly leverage LLMs to generate or augment tabular annotations at scale~\cite{hu2024annotatedtables}, reinforcing the need for scalable pipelines with traceable evidence and explicit constraints.

\section{Prompt Engineering and Constrained Generation for Reliable Outputs}

Prompt engineering steers LLM behavior without task-specific training. Chain-of-thought (CoT) prompting induces intermediate reasoning that improves multi-step inference and stabilizes decisions by making evidence integration explicit~\cite{wei2022chain}. Self-consistency enhances robustness by sampling multiple reasoning paths and selecting the most consistent answer, effectively ensembling generations at inference time~\cite{wang2022self}.

Beyond soft prompting, a growing body of work investigates constrained generation, where decoding is restricted to outputs satisfying formal constraints. Grammar-constrained decoding increases syntactic correctness and improves downstream accuracy in tasks requiring structured or executable outputs~\cite{raspanti2025grammar}. With JSON Schema emerging as a common specification for structured outputs, JSONSchemaBench and related evaluation efforts analyze the reliability, coverage, and efficiency of constrained decoding frameworks, demonstrating that constraint compliance can be improved but remains non-uniform across constraint types and implementations~\cite{geng2025jsonschemabench}.

Prompt-based steering and formal constraints thus offer complementary mechanisms for improving output reliability, motivating constraint-aware LLM pipelines for ontology-grounded table annotation in the following chapters.
